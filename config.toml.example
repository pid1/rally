# Rally Configuration File
# Copy this to config.toml and fill in your values

# Local timezone for displaying calendar events and scheduling
# This affects:
#   - Calendar event display times
#   - Scheduled dashboard generation (4:00 AM in this timezone)
# Use IANA timezone database names (e.g., "America/Chicago", "America/New_York", "America/Los_Angeles")
# Find your timezone: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
local_timezone = "America/Chicago"

[calendars]
# Get these from Google Calendar: Settings > Integrate Calendar > Secret address in iCal format
google_family = "https://calendar.google.com/calendar/ical/YOUR_CALENDAR_ID/private-SECRET/basic.ics"

# Get these from iCloud: Calendar.app > Right-click calendar > Share Calendar > Public Calendar
icloud_parent1 = "https://p01-caldav.icloud.com/published/2/YOUR_ICAL_URL"

# Add more calendars as needed:
# google_work = "https://..."
# icloud_parent2 = "https://..."
# google_kids_activities = "https://..."

# Optional: map calendar keys to the owner's email address.
# This lets Rally accurately detect which events YOU declined vs. events
# where a different attendee declined. Without this, Rally uses conservative
# heuristics (only hides events where ALL attendees declined or the event
# is cancelled).
[calendar_owners]
# google_family = "you@gmail.com"
# icloud_parent1 = "you@icloud.com"

[weather]
# Sign up at https://openweathermap.org/api (free tier: 1000 calls/day)
api_key = "your_openweather_api_key_here"

# Your home coordinates (find at https://www.latlong.net/)
# Example coordinates below are for Dallas, TX - replace with your location
lat = 32.7767
lon = -96.7970

[llm]
# Which provider to use: "local" or "anthropic"
# Switch this value to change which LLM backend Rally uses.
provider = "local"

[llm.local]
# OpenAI-compatible API endpoint
# Works with LM Studio, Ollama, vLLM, OpenAI, or any OpenAI-compatible server
#
# LM Studio:  http://YOUR_IP:1234/v1
# Ollama:     http://YOUR_IP:11434/v1
# OpenAI:     https://api.openai.com/v1
base_url = "http://localhost:1234/v1"

# API key (use "lm-studio" for LM Studio, "ollama" for Ollama, or your real key for OpenAI)
api_key = "lm-studio"

# Model name (must match what's loaded in your server)
# LM Studio: check the model name in the Developer tab
# Ollama: e.g. "llama3", "mistral"
# OpenAI: e.g. "gpt-4o"
model = "your-model-name"

[llm.anthropic]
# Anthropic API (uses native anthropic library)
# Get your key at https://console.anthropic.com/
api_key = "your-anthropic-api-key"
model = "claude-sonnet-4-20250514"
